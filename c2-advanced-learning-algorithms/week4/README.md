# Decision trees

This week, you'll learn about a practical and very commonly used learning algorithm the decision tree. You'll also learn about variations of the decision tree, including random forests and boosted trees (XGBoost).


# Decision trees

## Decision tree model

> <img src="./images/w04-01-Decision_tree_model/img_2023-02-05_19-25-09.png">

Here's an example of a model that you might get after training a decision tree learning algorithm on the data set that you just saw. 

Terminology:
- Every one of these ovals or rectangles is called a **node** in the tree.
- Top node is called **root node**
- ovals nodes are called **decision nodes** (beacaude, based on the features, they decide to go roght or left in the tree)
- Rectangle at the bottom are call **leaf nodes** and they make prediction

> <img src="./images/w04-01-Decision_tree_model/img_2023-02-05_19-27-00.png">

Among these different decision trees, some will do better and some will do worse on the training sets or on the cross-validation and test sets. The job of the decision tree learning algorithm is, out of all possible decision trees, to try to pick one that hopefully does well on the training set, and then also ideally generalizes well to new data such as your cross-validation and test sets as well

> <img src="./images/w04-01-Decision_tree_model/img_2023-02-05_19-36-17.png">

## Learning Process





# Practice quiz: Decision trees



# Decision trees learning

## Measuring purity

## Choosing a split: Information Gain

## Putting it together

## Using one-hot encoding of categorical features

## Continuous valued features

## Regression Trees (optional)

## Optional Lab: Decision Trees



# Practice quiz: Decision tree learning



# Tree ensembles

## Using multiple decision trees

## Sampling with replacement

## Random forest algorithm

## XGBoost

## When to use decision trees

## Optional Lab: Tree Ensembles



# Practice quiz: Tree ensembles




# Practice Lab: Decision Trees




# Acknowledgements

